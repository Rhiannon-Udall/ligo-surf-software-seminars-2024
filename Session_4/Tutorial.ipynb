{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c80ef-2cc5-4659-a871-b483472e033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical tools\n",
    "import numpy as np\n",
    "import scipy\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600ba3b-a928-4a1d-8aa0-9540a25939da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Background: What is a computer?  \n",
    "Modern hardware is optimized for very particular numerical operations, in general computers operate as *Turing Machines* which means they have a state (which is associated with the computer's memory), and actions which modify the state (these are called instructions).  An operating system is characterized by the set of instructions, and the default memory layout e.g. for an *x86-64* system the *instruction set* is x86, and memory addresses are most-naturally 64 bits (8 bytes).  As an aside, nowdays smaller or reduced instruction sets have become somewhat popular, for example new macbook computers and many smartphone use a type of reduced instruction set built by a company ARM (Advanced (Reduced instruction set computer) Machines) .  Programming languages allow people to write code which is converted to machine instructions by either a *compiler* (e.g. C, C++, Java(usually)) or an *interpreter* (Python(usually)).  Usually, but not always a program with fewer instructions will run faster than a program with many instructions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d77558-19d0-407e-a2e7-c6abff714bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3ae855-12af-49fa-aaae-19878c48214e",
   "metadata": {},
   "source": [
    "## Why do we care?\n",
    "Machines can only perform certain operations efficiently, because they have a limited number of operations which can be performed in a small number of instructions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fd817-09c2-4941-879c-17d6d81adf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Addition can be performed in a single instruction on modern machines\n",
    "# We can time how long it takes to perform the operation with the ipython magic command %timeit\n",
    "%timeit 1032421.1 + 123012.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692a3e0-bb3b-4444-93bb-83d8d357cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trig functions cannot be performed in a single instruction usually\n",
    "# They usually take much much longer than other operations\n",
    "%timeit np.sin(1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441024d4-2633-46a5-bac5-52c6a52bb391",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Most modern machines have efficient strategies for evaluating multiple floating point operations at once (called vectorization).  Generally it is faster to vectorize your code if you can.  In python, almost every `numpy` operation is vectorized, therefore you should always use numpy arrays, `np.array([...])` (and not python lists `[...]`!) when writing numeric code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fda21-cfcb-4c0b-9de7-b70b16505407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example\n",
    "x = np.arange(10000)\n",
    "%timeit y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba8012-ab6c-4781-a491-559b0f2e0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to\n",
    "x = np.arange(10000)\n",
    "%timeit y = [x[i]**2 for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514ed5e-7b85-432a-a895-5d7605fd778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second approach is almost 1000x slower!\n",
    "# Some operations may be surprisingly slow: for example compare the following to the cell above\n",
    "x = np.arange(10000)\n",
    "%timeit y = x ** 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f2855-3613-422a-9c48-8e242c140f4f",
   "metadata": {},
   "source": [
    "By just changing the exponent to 2.1 from 2 we have made the code run almost 100x slower.  This is because computers can do $x^2$ efficiently by rewriting it as the instruction $x * x$, but in order to evaluate $x^{2.1}$ it has to run a relativily sophisticated algorithm that rewrites the expression as $e^{2.1 \\ln(x)}$ and evaluate both a log and and an exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7875cb6-3845-4fc7-8cb5-c22b6e46bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful what ways you call functions! In many cases there is a special version of a function which is faster than a more general version\n",
    "%timeit np.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17334a9c-dbd9-428e-a3b8-a15626304674",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit x**(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f49962-2b15-4aa8-81fd-93086fc361e6",
   "metadata": {},
   "source": [
    "Writing efficient code is good for you and also for people who use your code (who spend less time waiting for code to run), but also is good for the planet.  A CPU takes somewhere around 10 watts to run.  A single CPU running all day will require the emission of about 100 g of $\\rm{CO}_2$; many high performance codes require 10's of thousands of CPU hours to run.  Well written, high-performance code can easily prevent a ton of $\\rm{CO}_2$ from entering the atmosphere over the lifetime of the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39629f-869d-4940-9b8a-525a15db202f",
   "metadata": {},
   "source": [
    "# Solving Linear Equations\n",
    "The simplest equation we can solve on a computer is something of the form `b*x + c = y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adffb70-105e-4792-9d89-65c619a6f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We solve equations by finding a value for the unknowns that make them true\n",
    "# Set up a specific problem\n",
    "b = 2.0\n",
    "c = 3.0\n",
    "y = 5.0\n",
    "# Solving step\n",
    "x = (y - c) / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e196c7-b80b-4037-8328-2e56daaef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case x is\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10a093-06c4-44fa-a7ce-ef9e38025c40",
   "metadata": {},
   "source": [
    "More generally, if $y=0$ then \n",
    "$$x=-\\frac{c}{b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35650548-a0ed-4178-bbc6-f52735d4a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It turns out this is basically the only problem that computers can solve (with a few caveats)\n",
    "# But what about quadratic equations! Surely computers can solve them (we'll just ask ChatGPT to write us a code that solves them)\n",
    "import numpy as np\n",
    "\n",
    "def solve_quadratic(a, b, c):\n",
    "    # Calculate the discriminant\n",
    "    discriminant = b**2 - 4*a*c\n",
    "    \n",
    "    # Check if the discriminant is positive, negative, or zero\n",
    "    if discriminant > 0:\n",
    "        # Two real and different roots\n",
    "        root1 = (-b + np.sqrt(discriminant)) / (2*a)\n",
    "        root2 = (-b - np.sqrt(discriminant)) / (2*a)\n",
    "        return root1, root2\n",
    "    elif discriminant == 0:\n",
    "        # One real root\n",
    "        root = -b / (2*a)\n",
    "        return root\n",
    "    else:\n",
    "        # Two complex roots\n",
    "        real_part = -b / (2*a)\n",
    "        imaginary_part = np.sqrt(-discriminant) / (2*a)\n",
    "        root1 = complex(real_part, imaginary_part)\n",
    "        root2 = complex(real_part, -imaginary_part)\n",
    "        return root1, root2\n",
    "\n",
    "# Example usage:\n",
    "a = 1\n",
    "b = -3\n",
    "c = 2\n",
    "\n",
    "roots = solve_quadratic(a, b, c)\n",
    "print(\"The roots of the quadratic equation are:\", roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256d87d-1d69-4153-b29e-e12394119c25",
   "metadata": {},
   "source": [
    "The Quadratic formula can actually be seen to reduce to the expression for a linear function above when a\n",
    "(the coefficient on the quadratic term) is small compared to b^2/c.  Concretely\n",
    "$$x = \\frac{-b \\pm \\sqrt{b^2 - 4 a c}}{2a}$$\n",
    "Taylor exapandng assuming $4ac \\ll b^2$\n",
    "\n",
    "$$\\sqrt{b^2 - 4ac} =  b\\sqrt{1-\\frac{4ac}{b^2}} \\approx b(1 - \\frac{2ac}{b^2})$$\n",
    "So the entire expression becomes \n",
    "$$x = \\frac{b \\pm (b -  \\frac{2ac}{b})}{2a}$$\n",
    "The \"+\" root will go to infinity as $a$ gets small, so we want the \"-\" root, which will become the solution of the equation in the limit.  \n",
    "This becomes \n",
    "$$x = \\frac{2ac}{2ab} = -\\frac{c}{b}$$\n",
    "Exactly what we expected! Let's use the code ChatGPT wrote to show how one of the roots converges to this value as a get's small!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0dd686-91e8-403b-af13-d1947f3938a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use Chat GPT's code to see this limiting happen in practice!\n",
    "# -b/c = -2.0, we just have to make a really small!\n",
    "b = 2.0\n",
    "c = 1.0\n",
    "a = 1e-1\n",
    "roots = solve_quadratic(a, b, c)\n",
    "print(\"The roots of the quadratic equation are:\", roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06493-c88b-4a92-bd48-48abf3ba0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uh oh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c9e82-ccc6-4df5-878e-255e84f3d9fe",
   "metadata": {},
   "source": [
    "## Solving Linear systems\n",
    "One of the most important operations modern computers perform today is solving systems of the form \n",
    "$$Ax=b$$ with $A$ a matrix and $x$ and $b$ vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06226ef-5101-4cce-975c-b743099af191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For something like this, it's always best to find a library to solve the problem\n",
    "# We'll use the numpy linalg library\n",
    "\n",
    "# make a matrix out of random gaussian draws\n",
    "A = np.random.randn(5,5)\n",
    "b = np.array([1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0f838-7ff8-49bb-a16c-2f0a5ddf2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linalg.solve(A, b)\n",
    "print(\"x = \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77356f6-8dee-404c-8a81-e2edbe3788f4",
   "metadata": {},
   "source": [
    "What about the matrix inverse?  \n",
    "many will have learned to solve the above system as \n",
    "$$x = A^{-1}b$$\n",
    "Generally, though, this is slower than using a function like `np.linalg.solve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0abf80-83eb-409c-a7b8-e45a338fbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate on a bigger problem\n",
    "A = np.random.randn(2000,2000)\n",
    "b = np.zeros(2000)\n",
    "b[0] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282f5e7-f0af-4c88-a416-c8bd4033ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit x = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a7d72-4490-48f5-9e99-0c5e0d25f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hm why does this seem so fast!!?\n",
    "%timeit x = A**-1 * b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1f0f-c0a0-41f9-8689-a4945c8a610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a trap! The above computation is not computing the right thing (it's doing everything pointwise)! \n",
    "# We need to treat the arrays as matrices, which numpy will not do automatically\n",
    "%timeit x = np.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb72b60-6ef0-4b23-a38a-5770e1a58b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The speedup of np.linalg.solve() vs inversion and multiplication should be, from theory a little larger than a factor of 2, which is\n",
    "# about what we see here.  If you need to solve a system 3 times with a different value of b, but the same value of A, use inversion.\n",
    "%timeit for i in range(5): np.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aed6f1-d4bf-4e0d-9282-c25eca63c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inversion is the only expensive step, each additional iteration is effectively free\n",
    "print(\"inverting matrix...\")\n",
    "%timeit A_inv = np.linalg.inv(A)\n",
    "print(\"done\")\n",
    "A_inv = np.linalg.inv(A) \n",
    "print(\"solving 5 times...\")\n",
    "%timeit  for i in range(5): A_inv @ b\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b4f56-0ce2-4739-a3e7-eb5e073412dc",
   "metadata": {},
   "source": [
    "# Solving Nonlinear Equations on computers\n",
    "Computers can't solve nonlinear equations :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641ad7b-8bf2-4279-9f98-764fc1c4870a",
   "metadata": {},
   "source": [
    "jk, while computers can't solve nonlinear equations directly, almost all modern computational physics (including numerical relativity)\n",
    "are about the study of how to approximate the solutions to nonlinear equations on computers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f802c65-5d40-418f-9e35-adb3844e84f3",
   "metadata": {},
   "source": [
    "By far the most common strategy for solving nonlinear equations is to approximate them by linear equations and solve the linear equations.  The process of approximating a nonlinear function by a linear function is called *calculus* :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2d7fd-2b0a-4901-a788-96d93d854f84",
   "metadata": {},
   "source": [
    "The simplest example of solving a nonlinear equation this way was invented by one of the originators of calculus: Newton's method.\n",
    "Newton's method approximates a function $f(x)$ by it's derivative and solves an equation using a linear approximation to the function\n",
    "rather than the function itself.  If we have an initial guess $x_0$ for the solution to an equation, i.e.\n",
    "$$f(x) = 0$$\n",
    "We note that the guess is likely not perfect, so $f(x_0) \\neq 0$.  What we do is we build a linear function that approximates $f(x)$ near $x_0$\n",
    "$$f_1(x) = f'(x_0)x  + f(x_0)$$\n",
    "This function has the same value and slope as $f(x)$ at $x=x_0$, therefore it's probably close to the function so long as $x$ is close to $x_0$.  \n",
    "We instead find the zero of this function\n",
    "$$f_1(x) = f'(x_0)(x-x_0) + f(x_0) = 0$$\n",
    "Which we can solve using the tools above\n",
    "$$x_1 = -f(x_0)/f'(x_0) + x_0 $$\n",
    "We label the solution of this equation $x_1$, now note $f(x_1)$ is probably not equal to zero, but it is likely closer than $f(x_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd97a3-3acb-40e5-803f-2d09f0855822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo of this algorithm in practice\n",
    "# The function and it's derivative are known analytically\n",
    "def f(x):\n",
    "    return np.sin(x) - 0.652\n",
    "def f_prime(x):\n",
    "    return np.cos(x)\n",
    "# choose some initial guess\n",
    "x_0 = 1.2 * np.pi/4\n",
    "# set the current guess to the initial guess\n",
    "x_guess = x_0\n",
    "# Make a plot for each iteration\n",
    "N_iters = 5\n",
    "fig, axs = plt.subplots(N_iters, 1, sharex=True,figsize=(6, 24))\n",
    "\n",
    "x_vals = np.linspace(np.pi/6,  np.pi/3)\n",
    "for i in range(N_iters):\n",
    "    # Plot the function\n",
    "    axs[i].plot(x_vals, np.sin(x_vals) - 0.652, color=\"navy\", label=r\"$f(x)$\")\n",
    "    # Plot line x=0\n",
    "    axs[i].plot(x_vals, np.zeros_like(x_vals))\n",
    "    # Plot the x-value of the guess\n",
    "    axs[i].axvline(x_guess, color=\"red\")\n",
    "    # Plot the approximating function\n",
    "    axs[i].plot(x_vals, f(x_guess) + f_prime(x_guess) * (x_vals-x_guess), color=\"deepskyblue\", label=rf\"$f_{i+1}(x)$\")\n",
    "    axs[i].legend()\n",
    "    # Update the guess\n",
    "    x_guess = -f(x_guess)/f_prime(x_guess) + x_guess\n",
    "plt.legend()    \n",
    "plt.plot()\n",
    "print(\"final estimate x_guess = \", x_guess)\n",
    "print(\"function value f(x_guess) = \", f(x_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c275e3-a25c-48bd-a9a6-c71284c3c9fe",
   "metadata": {},
   "source": [
    "After only 3 iterations the location of the aproximate zero is already exquisitely good, after 5 iterations the zero is found to greater than machine precision.  When Newton's method converges, it *superconverges*: the number of correct digits doubles with each iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9cf15c-6771-493e-8d6c-b0d2e71b2df5",
   "metadata": {},
   "source": [
    "Many Nonlinear problems are solved in a similar manner, optimization is a highly related topic which is now very much in vogue, and one such \n",
    "algorithm for optimization is called *gradient descent*, which is nearly identical to Newton's method.  Modern approaches typically build on \n",
    "these methods to make them more robust and get them to converge faster, in general you should use a nonlinear solver from a library, such \n",
    "as `scipy.optimize.root`, which takes a function $f(x)$ (possibly vector valued), and an initial guess $(x_0)$ and returns a root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bbfb9-31e8-4cdd-a0d2-07e74b5967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes a function callable (i.e. a function) and an initial guess, and some other optional arguments \n",
    "# (including the jacobian which in this case is just the derivative) we don't need to supply the jacobian, but it can improve \n",
    "# performance if we know it (otherwise it has to construct it numerically by calling the function many times)\n",
    "?scipy.optimize.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df647ea-9d77-4722-9df4-e147b6797ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main thing to notice is the \"x\"-value which is output (along with a bunch of other useful information)\n",
    "# You'll see it agrees with the value we found above\n",
    "found_root = scipy.optimize.root(f, x0 = x_0, jac=f_prime)\n",
    "print(found_root)\n",
    "print(\"x = \", found_root.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50180b65-8131-49ab-86df-08b944ebc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also works\n",
    "scipy.optimize.root(f, x0 = x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c43fc0-0174-42ea-a76d-da0daf0c2294",
   "metadata": {},
   "source": [
    "## Caution!\n",
    "If a function has multiple roots, there's no guarantee that Newton's method will converge to any particular root.  If \n",
    "you need a *specific* root of a function, you may have to investigate other numerical methods, or place bounds on the domain\n",
    "you are searching for the solution in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce710770-2490-4c88-b5f9-3f8bdd751f2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Solving Differential Equations on computers\n",
    "Computers can't solve differential equations :(("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01de4f-04a7-4838-8a84-197dcfbeb2bd",
   "metadata": {},
   "source": [
    "Well ok, maybe Wolfram Alpha can, but in general there will be differential equations sufficiently complicated to not have\n",
    "analytic solutuions.  In these cases, the solution to the differnetial equation must be constructed by approximating the problem\n",
    "of solving the differntial equation by a solving a sequence of linear equations. Typically the solution to a differential equation \n",
    "is constructed via a sequence of steps, where the function is approximated by it's derivative (sound familiar?).  Although exactly how \n",
    "this is done can substantially change how quickly and accurately the differntial equation is solved.  In general if an ODE solver seems to be taking a long time, google the word \"Stiff\" and try a different  solver.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138ef42-9845-4339-8b93-0b761aa49e43",
   "metadata": {},
   "source": [
    "For example, if we want to solve an ordinary differential equation of the form\n",
    "$$y'(t) = f(t,y)$$\n",
    "With an initial condition $y(t_0) = y_0$\n",
    "the best thing to do is to use an implemented ode solver.  We'll use an implementation from the \n",
    "`scipy.integrate` library.  This is an initial value problem (IVP), so we will use \n",
    "`scipy.integrate.solve_ivp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569526a-78b8-4d7a-b273-efe33759ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "?scipy.integrate.solve_ivp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3d366-594a-4dc7-83c9-c62f985aefbb",
   "metadata": {},
   "source": [
    "By default `solve_ivp` uses a particular numerical method `RK45` to solve the differntial equation.  This is a 4th order Runge-Kutta method \n",
    "(which means steps are taken by effectively taylor expanding a function to 4th order) and errors are controlled by comparing to a 5th\n",
    "order method.  This is a good default to use, but in certain cases (stiff systems), a different solver should be used.  It's often not \n",
    "possible to know what solver will work before trying it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5aadd4-9792-425d-958f-4b6894a76426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well solve a simple just-barely-nonlinear differential equation\n",
    "def f(t, y):\n",
    "    return -y + 0.1 * y**3\n",
    "# We'll want to solve the equation over some range\n",
    "# almost always, we'll want to use 'dense output' which means\n",
    "# that we'll ask the solver to output the solutuon to the ODE \n",
    "# at intermediate times between the initial and final time.  \n",
    "# we specify which times this will be by using the `t_eval` argument\n",
    "\n",
    "# Where we solve over\n",
    "t_span = (0, 3.0)\n",
    "# Where we want to output the solution at  \n",
    "# (the * notation means to expand the tuple and )\n",
    "# and treat it as arguments to a function\n",
    "t_eval = np.linspace(*t_span, 30)\n",
    "\n",
    "# Just for making coloring lines easily\n",
    "import matplotlib.cm as cm\n",
    "for i, y_0  in enumerate([1.0, 2.0, 3.0, 3.1, 3.15, 3.18]):\n",
    "    # This is where the ODE is actually solved\n",
    "    solution = scipy.integrate.solve_ivp(fun=f, t_span=t_span, y0=np.array([y_0]), dense_output=True, t_eval=t_eval)\n",
    "    plt.plot(solution.t, solution.y[0,:], label=f\"y_0 = {y_0}\", color=cm.viridis(i/5))\n",
    "plt.ylabel(\"y(t)\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed14c0a-03ba-4bab-a75a-912e979a7ca5",
   "metadata": {},
   "source": [
    "## Challenge Problem\n",
    "Use `solve_ivp` to find the solution to a general 1-d Newtonian dynamics problem. That is, assume there is an object with mass  $m$ \n",
    "moving under the influence of some net force $F$, with a velocity $v = dx/dt$ (say it starts out with an initial velocity $v_0$, at an initial position $x_0$)\n",
    "Hint: You'll need to define a *state* vector $Z = (x(t), v(t))$, and give a vector equation for the evolution of the state to the  IVP solver.  I.e. you'll need\n",
    "$$\\frac{dZ}{dt} = \\left(\\frac{dx}{dt}, \\frac{dv}{dt}\\right)$$\n",
    "And you'll need to express $dx/dt$ and $dv/dt$ in terms of the state variables $(t, x, v)$, and $F(t,x,v)$ the force\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac36f1f-232f-411a-9691-1ead1711a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_by_dt(t, Z, F):\n",
    "    x = Z[0]\n",
    "    v = Z[1]\n",
    "    F_applied = F(t, x, v)\n",
    "    # TODO your code here\n",
    "    return None\n",
    "\n",
    "def F_spring(t, x, v):\n",
    "    return -x\n",
    "\n",
    "# Define variables needed for the solve_ivp call\n",
    "\n",
    "\n",
    "# Below, the syntax \"lambda\" just defines a function without a name and returns it (called an anonymous function)\n",
    "# It makes it easier to make functions more generic when defined so later values cal be substituted into them\n",
    "# Note lambdas automatically return, but do not explicitly use the `return` keyword\n",
    "\n",
    "solution_Z = scipy.integrate.solve_ivp(lambda t, Z: dZ_by_dt(t, Z, F_spring), ) # Complete this function call with your own code\n",
    "\n",
    "# Make a plot of the solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949d237-79e5-4f3e-bda2-ee2628ddc961",
   "metadata": {},
   "source": [
    "## Double challenge problem\n",
    "Write code to solve an N-d Newton problem (i.e. a newtonian mechanics problem with N degrees of freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b967302-43cf-4a3b-9b47-ac1287dbcad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": " igwn-py",
   "language": "python",
   "name": "igwn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
